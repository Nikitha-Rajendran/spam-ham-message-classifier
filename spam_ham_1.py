# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q76Hc4i6HZEwfz1gYgrRClBWCLElrWUy
"""

import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk import stem
from nltk.corpus import stopwords
from nltk import pos_tag, word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from sklearn.metrics import confusion_matrix

data = pd.read_csv("/content/drive/MyDrive/topicmodelling/spam.csv", encoding = "latin-1")
data = data[['v1', 'v2']]
data = data.rename(columns = {'v1': 'label', 'v2': 'text'})
print(data)

nltk.download('stopwords')

stemmer = stem.SnowballStemmer('english')
stopwords = set(stopwords.words('english'))

def review_messages(msg):
    msg = msg.lower()
    msg = [word for word in msg.split() if word not in stopwords]
    msg = " ".join([stemmer.stem(word) for word in msg])
    return msg

data['text'] = data['text'].apply(review_messages)
print(data['text'])

X_train, X_test, Y_train, Y_test = train_test_split(data['text'], data['label'], test_size = 0.1, random_state = 1)

vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(X_train)

svm = svm.SVC(C=1000)
svm.fit(X_train, Y_train)

X_test = vectorizer.transform(X_test)
Y_pred = svm.predict(X_test)
print(confusion_matrix(Y_test, Y_pred))

def pred(msg):
    msg = vectorizer.transform([msg])
    prediction = svm.predict(msg)
    return prediction[0]

trial="""Daily Deals showcase our biggest discounts, and we launch over a dozen each week.
 
Want to be sure you see them all? Manage you preferences via the link below to make sure you get access to all of our best deals on t-shirts, vinyl and more."""

trial=review_messages(trial)

print(trial)

trial=pred(trial)

print(trial)